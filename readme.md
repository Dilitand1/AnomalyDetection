DeepAnomalyDetection

Вот пошаговое объяснение каждой строки в блоке конфигурации автоэнкодера:

```java
MultiLayerNetwork model = new MultiLayerNetwork(
    new NeuralNetConfiguration.Builder()
        // 1. Инициализация генератора случайных чисел для воспроизводимости
        .seed(123) 

        // 2. Настройка оптимизатора (Adam с learning rate = 0.01)
        .updater(new Adam(0.01)) 

        // 3. Метод инициализации весов (Xavier для равномерного распределения)
        .weightInit(WeightInit.XAVIER) 

        // 4. Начало описания слоев сети
        .list() 

        // 5. Первый скрытый слой (энкодер)
        .layer(new DenseLayer.Builder()
            .nIn(1)                // 5.1. 1 входной нейрон (значение)
            .nOut(3)              // 5.2. 3 нейрона в слое (сжатие)
            .activation(Activation.RELU) // 5.3. Функция активации
            .build()) 

        // 6. Выходной слой (декодер)
        .layer(new OutputLayer.Builder()
            .nIn(3)                // 6.1. 3 входа (из предыдущего слоя)
            .nOut(1)               // 6.2. 1 выход (реконструкция значения)
            .lossFunction(LossFunctions.LossFunction.MSE) // 6.3. Функция потерь
            .activation(Activation.IDENTITY) // 6.4. Линейная активация
            .build()) 

        // 7. Финальная сборка конфигурации
        .build() 
);

// 8. Инициализация весов сети
model.init(); 
```

### Детализация параметров:
1. **Скрытый слой (энкодер)**:
   - `nIn(1)` — принимает 1 входное значение (нормализованное число)
   - `nOut(3)` — сжимает данные в 3-мерное представление
   - `RELU` — нелинейная активация для обучения сложным паттернам

2. **Выходной слой (декодер)**:
   - `nIn(3)` — получает данные из скрытого слоя (3 значения)
   - `nOut(1)` — восстанавливает исходное измерение
   - `MSE` — оптимальная функция потерь для регрессии
   - `IDENTITY` — линейная активация для сохранения масштаба значений

3. **Важные нюансы**:
   - Архитектура 1-3-1 специально выбрана для создания "бутылочного горлышка"
   - Xavier-инициализация помогает избежать проблем с градиентами
   - Adam оптимизирует скорость обучения для каждого параметра

Это минимальная рабочая конфигурация. В реальных задачах можно:
- Увеличить количество слоев
- Добавить регуляризацию (например, `.l2(0.001)`)
- Настроить размер скрытого представления
- Экспериментировать с функциями активации